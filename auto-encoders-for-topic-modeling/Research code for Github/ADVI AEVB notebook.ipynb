{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVI AEVB\n",
    "\n",
    "The original Notebook from which my research was based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu,floatX=float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "# unfortunately I was not able to run it on GPU due to overflow problems\n",
    "%env THEANO_FLAGS=device=cpu,floatX=float64\n",
    "import theano\n",
    "\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from theano import shared\n",
    "import theano.tensor as tt\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "import pymc3 as pm\n",
    "from pymc3 import math as pmmath\n",
    "from pymc3 import Dirichlet\n",
    "from pymc3.distributions.transforms import t_stick_breaking\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Articles (big) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'.//datasets/Medium_Clean.csv' does not exist: b'.//datasets/Medium_Clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5749f17d1678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".//datasets/Medium_Clean.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchunk_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# append each chunk df here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Each chunk is in df format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'.//datasets/Medium_Clean.csv' does not exist: b'.//datasets/Medium_Clean.csv'"
     ]
    }
   ],
   "source": [
    "df_chunk = pd.read_csv(e\"datasets/Medium_Clean.csv\", chunksize=1000000)\n",
    "    \n",
    "chunk_list = []  # append each chunk df here \n",
    "\n",
    "# Each chunk is in df format\n",
    "for chunk in df_chunk:  \n",
    "    # perform data filtering \n",
    "   # chunk_filter = chunk_preprocessing(chunk)\n",
    "    chunk_filter = chunk\n",
    "    \n",
    "    # Once the data filtering is done, append the chunk to list\n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "# concat the list into dataframe \n",
    "df_concat = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_concat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0d00f2a7be0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_concat' is not defined"
     ]
    }
   ],
   "source": [
    "df_concat.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_concat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-af755aae1cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for now just work with 'Title'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmedium_stories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# drop empty rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmedium_stories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedium_stories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_concat' is not defined"
     ]
    }
   ],
   "source": [
    "# for now just work with 'Title'\n",
    "medium_stories = df_concat['Title']\n",
    "\n",
    "# drop empty rows\n",
    "medium_stories = medium_stories.dropna()\n",
    "\n",
    "# convert to list\n",
    "medium_stories = medium_stories.tolist()\n",
    "\n",
    "medium_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon fine food reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 'B001E4KFG0' 'A3SGXH7AUHU8GW' ... 1303862400 'Good Quality Dog Food'\n",
      "  'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.']\n",
      " [2 'B00813GRG4' 'A1D87F6ZCVE5NK' ... 1346976000 'Not as Advertised'\n",
      "  'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".']\n",
      " [3 'B000LQOCH0' 'ABXLMWJIXXAIN' ... 1219017600 '\"Delight\" says it all'\n",
      "  'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n",
      " ...\n",
      " [568452 'B004I613EE' 'A121AA1GQV751Z' ... 1329782400\n",
      "  'Perfect for our maltipoo'\n",
      "  'These stars are small, so you can give 10-15 of those in one training session.  I tried to train our dog with \"Ceaser dog treats\",  it just made our puppy hyper.  If you compare the ingredients, you will know why.  Little stars has just basic food ingredients without any preservatives and food coloring.  Sweet potato flavor also did not make my hand smell like dog food.']\n",
      " [568453 'B004I613EE' 'A3IBEVCTXKNOH' ... 1331596800\n",
      "  'Favorite Training and reward treat'\n",
      "  'These are the BEST treats for training and rewarding your dog for being good while grooming.  Lower in calories and loved by all the doggies.  Sweet potatoes seem to be their favorite Wet Noses treat!']\n",
      " [568454 'B001LR2CU2' 'A3LGQPJCZVL9UC' ... 1338422400 'Great Honey'\n",
      "  'I am very satisfied ,product is as advertised, I use it on cereal, with raw vinegar, and as a general sweetner.']]\n"
     ]
    }
   ],
   "source": [
    "df_chunk = pd.read_csv(r\"../datasets/amazon-fine-food-reviews/Reviews.csv\", chunksize=50000)\n",
    "    \n",
    "chunk_list = []  # append each chunk df here \n",
    "\n",
    "# Each chunk is in df format\n",
    "for chunk in df_chunk:  \n",
    "    # perform data filtering \n",
    "   # chunk_filter = chunk_preprocessing(chunk)\n",
    "    chunk_filter = chunk\n",
    "    \n",
    "    # Once the data filtering is done, append the chunk to list\n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "# concat the list into dataframe \n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(df_concat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have reviews with scores and summaries, we are going to use the 'text' column of the review and the 'scores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores\n",
    "scores = df_concat['Score']\n",
    "\n",
    "# reviews\n",
    "reviews = df_concat['Text']\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 99.385s.\n"
     ]
    }
   ],
   "source": [
    "# use if you using new dataset\n",
    "\n",
    "# The number of words in the vocabulary\n",
    "n_words = 1000\n",
    "\n",
    "# input data\n",
    "data_samples = reviews\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_words,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.987s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.649s.\n"
     ]
    }
   ],
   "source": [
    "# The number of words in the vocabulary\n",
    "n_words = 1000\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Insert my datasets if required\n",
    "# data_samples = ted_talks['description']\n",
    "#data_samples = medium_stories\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_words,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each doc is represented by a 1000-dimensional term freq-vector\n",
    "# plt.plot(tf[:100, :].toarray().T);\n",
    "\n",
    "# tf: term frequency doc matrix\n",
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs for training = 7920\n",
      "Number of docs for testing = 3394\n",
      "Number of tokens in training set = 384502\n",
      "Sparsity = 0.0255030303030303\n"
     ]
    }
   ],
   "source": [
    "# We split the whole documents into training and test sets. \n",
    "# The number of tokens in the training set is 480K. \n",
    "# Sparsity of the term-frequency document matrix is 0.025%, which implies almost all components in the term-frequency matrix is zero\n",
    "\n",
    "\n",
    "n_samples_tr = round(tf.shape[0] * 0.7) # testing on 70%\n",
    "n_samples_te = tf.shape[0] - n_samples_tr\n",
    "docs_tr = tf[:n_samples_tr, :]\n",
    "docs_te = tf[n_samples_tr:, :]\n",
    "print('Number of docs for training = {}'.format(docs_tr.shape[0]))\n",
    "print('Number of docs for testing = {}'.format(docs_te.shape[0]))\n",
    "\n",
    "\n",
    "n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
    "print('Number of tokens in training set = {}'.format(n_tokens))\n",
    "print('Sparsity = {}'.format(\n",
    "    len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-likelihood of documents for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_lda_doc(beta, theta):\n",
    "  \n",
    "  \"\"\"Returns the log-likelihood function for given documents.\n",
    "  \n",
    "  K : number of topics in the model\n",
    "  V : number of words (size of vocabulary)\n",
    "  D : number of documents (in a mini-batch)\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  beta : tensor (K x V)\n",
    "      Word distribution.\n",
    "  theta : tensor (D x K)\n",
    "      Topic distributions for the documents.\n",
    "  \"\"\"\n",
    "  \n",
    "  def ll_docs_f(docs):\n",
    "    \n",
    "    dixs, vixs = docs.nonzero()\n",
    "    vfreqs = docs[dixs, vixs]\n",
    "    ll_docs = vfreqs * pmmath.logsumexp(\n",
    "          tt.log(theta[dixs]) + tt.log(beta.T[vixs]), axis=1).ravel()\n",
    "      \n",
    "    # Per-word log-likelihood times no. of tokens in the whole dataset\n",
    "    return tt.sum(ll_docs) / (tt.sum(vfreqs)+1e-9) * n_tokens\n",
    "\n",
    "  return ll_docs_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "\n",
    "\n",
    "# we have sparse dataset. It's better to have dence batch so that all words accure there\n",
    "minibatch_size = 128\n",
    "\n",
    "# defining minibatch\n",
    "doc_t_minibatch = pm.Minibatch(docs_tr.toarray(), minibatch_size)\n",
    "doc_t = shared(docs_tr.toarray()[:minibatch_size])\n",
    "\n",
    "with pm.Model() as model:\n",
    "    theta = Dirichlet('theta', a=pm.floatX((1.0 / n_topics) * np.ones((minibatch_size, n_topics))),\n",
    "                   shape=(minibatch_size, n_topics), transform=t_stick_breaking(1e-9),\n",
    "                   # do not forget scaling\n",
    "                   total_size = n_samples_tr)\n",
    "    beta = Dirichlet('beta', a=pm.floatX((1.0 / n_topics) * np.ones((n_topics, n_words))),\n",
    "                 shape=(n_topics, n_words), transform=t_stick_breaking(1e-9))\n",
    "        # Note, that we defined likelihood with scaling, so here we need no additional `total_size` kwarg\n",
    "    doc = pm.DensityDist('doc', logp_lda_doc(beta, theta), observed=doc_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAEncoder:\n",
    "  \"\"\"Encode (term-frequency) document vectors to variational means and (log-transformed) stds.\n",
    "  \"\"\"\n",
    "  def __init__(self, n_words, n_hidden, n_topics, p_corruption=0, random_seed=1):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    self.n_words = n_words\n",
    "    self.n_hidden = n_hidden\n",
    "    self.n_topics = n_topics\n",
    "    self.w0 = shared(0.01 * rng.randn(n_words, n_hidden).ravel(), name='w0')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class LDAEncoder:\n",
    "    \"\"\"Encode (term-frequency) document vectors to variational means and (log-transformed) stds.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_words, n_hidden, n_topics, p_corruption=0, random_seed=1):\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.n_words = n_words\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_topics = n_topics\n",
    "        self.w0 = shared(0.01 * rng.randn(n_words, n_hidden).ravel(), name='w0')\n",
    "        self.b0 = shared(0.01 * rng.randn(n_hidden), name='b0')\n",
    "        self.w1 = shared(0.01 * rng.randn(n_hidden, 2 * (n_topics - 1)).ravel(), name='w1')\n",
    "        self.b1 = shared(0.01 * rng.randn(2 * (n_topics - 1)), name='b1')\n",
    "        self.rng = MRG_RandomStreams(seed=random_seed)\n",
    "        self.p_corruption = p_corruption\n",
    "\n",
    "    def encode(self, xs):\n",
    "        if 0 < self.p_corruption:\n",
    "            dixs, vixs = xs.nonzero()\n",
    "            mask = tt.set_subtensor(\n",
    "                tt.zeros_like(xs)[dixs, vixs],\n",
    "                self.rng.binomial(size=dixs.shape, n=1, p=1-self.p_corruption)\n",
    "            )\n",
    "            xs_ = xs * mask\n",
    "        else:\n",
    "            xs_ = xs\n",
    "\n",
    "        w0 = self.w0.reshape((self.n_words, self.n_hidden))\n",
    "        w1 = self.w1.reshape((self.n_hidden, 2 * (self.n_topics - 1)))\n",
    "        hs = tt.tanh(xs_.dot(w0) + self.b0)\n",
    "        zs = hs.dot(w1) + self.b1\n",
    "        zs_mean = zs[:, :(self.n_topics - 1)]\n",
    "        zs_rho = zs[:, (self.n_topics - 1):]\n",
    "        return {'mu': zs_mean, 'rho':zs_rho}\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.w0, self.b0, self.w1, self.b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.641329326498145"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape\n",
    "np.mean(np.sum(tf, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(theta,\n",
       "              {'mu': Subtensor{::, :int64:}.0,\n",
       "               'rho': Subtensor{::, int64::}.0})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LDAEncoder(n_words=n_words, n_hidden=100, n_topics=n_topics, p_corruption=0.0)\n",
    "local_RVs = OrderedDict([(theta, encoder.encode(doc_t))])\n",
    "local_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[w0, b0, w1, b1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_params = encoder.get_params()\n",
    "encoder_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEVB with ADVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 41 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4674e+06: 100%|██████████| 10000/10000 [05:23<00:00, 30.91it/s]\n",
      "Finished [100%]: Average Loss = 2.469e+06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymc3.variational.approximations.MeanField at 0x1c2bf5d2e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "η = .1\n",
    "s = shared(η)\n",
    "def reduce_rate(a, h, i):\n",
    "    s.set_value(η/((i/minibatch_size)+1)**.7)\n",
    "\n",
    "with model:\n",
    "    approx = pm.MeanField(local_rv=local_RVs)\n",
    "    approx.scale_cost_to_minibatch = False\n",
    "    inference = pm.KLqp(approx)\n",
    "inference.fit(10000, callbacks=[reduce_rate], obj_optimizer=pm.sgd(learning_rate=s),\n",
    "              more_obj_params=encoder_params, total_grad_norm_constraint=200,\n",
    "              more_replacements={doc_t:doc_t_minibatch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation{MeanFieldGroup[None, 19] & MeanFieldGroup[19980]}\n"
     ]
    }
   ],
   "source": [
    "print(approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of characteristic words of topics based on posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: people god don think just know like say said time\n",
      "Topic #1: edu use file key program information space available data ftp\n",
      "Topic #2: just don like good think time year car know game\n",
      "Topic #3: windows drive use card disk problem thanks does scsi like\n",
      "Topic #4: know like don just does edu thanks good use think\n",
      "Topic #5: 00 10 55 20 11 15 12 17 16 18\n",
      "Topic #6: know like don just does edu thanks good use think\n",
      "Topic #7: like know just don does edu thanks good new use\n",
      "Topic #8: know like don just does edu thanks new think good\n",
      "Topic #9: ax max g9v b8f a86 75u bhj 1t pl 145\n",
      "Topic #10: like know don just does edu thanks good think new\n",
      "Topic #11: like know don just does edu good thanks new think\n",
      "Topic #12: know like just don does edu good thanks new think\n",
      "Topic #13: db bh cs al bits cx just know like don\n",
      "Topic #14: like just don know does edu good new thanks think\n",
      "Topic #15: like don just know does edu good thanks people think\n",
      "Topic #16: like don just know does edu good make com thanks\n",
      "Topic #17: don just know like does edu new good com people\n",
      "Topic #18: don just like know good does edu did com people\n",
      "Topic #19: just does like don edu know got good help did\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    for i in range(len(beta)):\n",
    "        print((\"Topic #%d: \" % i) + \" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "doc_t.set_value(docs_tr.toarray())\n",
    "samples = pm.sample_approx(approx, draws=100)\n",
    "beta_pymc3 = samples['beta'].mean(axis=0)\n",
    "\n",
    "print_top_words(beta_pymc3, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to sklearn LDA implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 159 ms, total: 31 s\n",
      "Wall time: 26 s\n",
      "Topic #0: gun people state control law right crime police guns states\n",
      "Topic #1: time like don question did book years just know think\n",
      "Topic #2: db science line rules bh current int define title lines\n",
      "Topic #3: key chip clipper keys use des encryption algorithm chips bit\n",
      "Topic #4: edu com cs cx mail vs gm article uk send\n",
      "Topic #5: does use point window problem value mean way different used\n",
      "Topic #6: windows thanks know dos help does like problem using use\n",
      "Topic #7: water bike effect design road dod paper inside turn hot\n",
      "Topic #8: just don think like know people good going ve time\n",
      "Topic #9: car price good power new used air ground sale wire\n",
      "Topic #10: file available program information ftp edu files use list anonymous\n",
      "Topic #11: ax max g9v pl b8f 75u a86 bhj 1t 34u\n",
      "Topic #12: law government privacy security legal encryption fbi court private enforcement\n",
      "Topic #13: color bit output memory video jpeg data image mode card\n",
      "Topic #14: space drive scsi disk hard mac launch drives controller apple\n",
      "Topic #15: god jesus people believe bible christian church does life faith\n",
      "Topic #16: year team game games hockey season league play players nhl\n",
      "Topic #17: 00 10 15 12 20 11 16 14 55 30\n",
      "Topic #18: armenian israel armenians jews turkish people israeli war jewish men\n",
      "Topic #19: mr said president stephanopoulos new people time day going health\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "%time lda.fit(docs_tr)\n",
    "beta_sklearn = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print_top_words(beta_sklearn, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pp(ws, thetas, beta, wix):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws: ndarray (N,)\n",
    "        Number of times the held-out word appeared in N documents.\n",
    "    thetas: ndarray, shape=(N, K)\n",
    "        Topic distributions for N documents.\n",
    "    beta: ndarray, shape=(K, V)\n",
    "        Word distributions for K topics.\n",
    "    wix: int\n",
    "        Index of the held-out word\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Log probability of held-out words.\n",
    "    \"\"\"\n",
    "    return ws * np.log(thetas.dot(beta[:, wix]))\n",
    "\n",
    "def eval_lda(transform, beta, docs_te, wixs):\n",
    "    \"\"\"Evaluate LDA model by log predictive probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transform: Python function\n",
    "        Transform document vectors to posterior mean of topic proportions.\n",
    "    wixs: iterable of int\n",
    "        Word indices to be held-out.\n",
    "    \"\"\"\n",
    "    lpss = []\n",
    "    docs_ = deepcopy(docs_te)\n",
    "    thetass = []\n",
    "    wss = []\n",
    "    total_words = 0\n",
    "    for wix in wixs:\n",
    "        ws = docs_te[:, wix].ravel()\n",
    "        if 0 < ws.sum():\n",
    "            # Hold-out\n",
    "            docs_[:, wix] = 0\n",
    "\n",
    "            # Topic distributions\n",
    "            thetas = transform(docs_)\n",
    "\n",
    "            # Predictive log probability\n",
    "            lpss.append(calc_pp(ws, thetas, beta, wix))\n",
    "\n",
    "            docs_[:, wix] = ws\n",
    "            thetass.append(thetas)\n",
    "            wss.append(ws)\n",
    "            total_words += ws.sum()\n",
    "        else:\n",
    "            thetass.append(None)\n",
    "            wss.append(None)\n",
    "\n",
    "    # Log-probability\n",
    "    lp = np.sum(np.hstack(lpss)) / total_words\n",
    "\n",
    "    return {\n",
    "        'lp': lp,\n",
    "        'thetass': thetass,\n",
    "        'beta': beta,\n",
    "        'wss': wss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tt.matrix(dtype='int64')\n",
    "sample_vi_theta = theano.function(\n",
    "    [inp],\n",
    "    approx.sample_node(approx.model.theta, 100,  more_replacements={doc_t: inp}).mean(0)\n",
    ")\n",
    "def transform_pymc3(docs):\n",
    "    return sample_vi_theta(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 3.51 s, total: 1min 27s\n",
      "Wall time: 1min 22s\n",
      "Predictive log prob (pm3) = -6.078433976876863\n"
     ]
    }
   ],
   "source": [
    "%time result_pymc3 = eval_lda(transform_pymc3, beta_pymc3, docs_te.toarray(), np.arange(100))\n",
    "print('Predictive log prob (pm3) = {}'.format(result_pymc3['lp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 31s, sys: 2.21 s, total: 3min 33s\n",
      "Wall time: 3min 5s\n",
      "Predictive log prob (sklearn) = -5.991822468667646\n"
     ]
    }
   ],
   "source": [
    "def transform_sklearn(docs):\n",
    "    thetas = lda.transform(docs)\n",
    "    return thetas / thetas.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "%time result_sklearn = eval_lda(transform_sklearn, beta_sklearn, docs_te.toarray(), np.arange(100))\n",
    "print('Predictive log prob (sklearn) = {}'.format(result_sklearn['lp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
