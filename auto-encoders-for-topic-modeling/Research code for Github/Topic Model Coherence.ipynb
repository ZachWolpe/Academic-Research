{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model Coherence\n",
    "\n",
    "```\n",
    "Author:\n",
    "    Zach Wolpe\n",
    "    zachcolinwolpe@gmail.com\n",
    "    zachwolpe.com\n",
    "```\n",
    "\n",
    "_Topic Coherence_ is a relative metric that provides a metric for determining:\n",
    "    - Performance descrepancies between topic models\n",
    "    - Finding the optimal number of topics \n",
    "    \n",
    "    \n",
    "    \n",
    "### Coherence Calculation\n",
    "\n",
    " $$\\text{Coherence} = \\sum_{i<j} score(w_i, w_j)$$\n",
    "\n",
    "Where words $W={w_1, w_2, ..., w_n}$ are ordered from most to least frequently appearing. The two leading coherence algorithms (UMass and UCI) essentially measure the same thing \\cite{newman2010automatic} and as such I chosen to focus on UMass. The UMass \\textit{scores} between $\\{w_i, w_j\\}$ combinations (which are summed subsequent to calculation) are computed as:\n",
    "\n",
    "$$score_{UMass}^{k}(w_i, w_j | K) = \\log \\frac{D(w_i, w_j)+ \\epsilon}{D(w_i)}$$\n",
    "\n",
    "Where $K_i$ is $i^{th}$ topic returned by the model and $w_i$ is more common than $w_j$. $D(w_i)$ is the probability of a word $w_i$ is in a document (the number of times $w_i$ appears in a document divided by total documents). $D(w_i, w_j)$ is the conditional probability that $w_j$ will occur in a document, given $w_i$ is in the document - which eludes to some sort of dependency between key words within a topic \\cite{mimno2011optimizing}. $\\epsilon$ simply provides a smoothing parameter, which is often simply set to $1$ to avoid taking $\\log(0)$ in the case where the conditional probability is zero. \n",
    "\n",
    "One concern arises, as it is overtly clear that the number of $w_i, w_j$ combinations balloons to absurd quantities for even even relatively small corpus documents with few words in the aggregated vocabulary. \n",
    "\n",
    "\n",
    "As such, it is adequate to simply compute coherence for word pairs $\\{(w_1,w_2),(w_2,w_3), ..., (w_{n-1},w_n)\\}$ \\cite{mimno2011optimizing}.\n",
    "\n",
    "To improve the reliability of computed coherence, we can factor in the sampling distribution of documents by learning the spread of topic coherence for a given dataset. This is achieved by: randomizing the train-test split; computing LDA parameters; calculating coherence on the returned topics; repeating many times. \n",
    "\n",
    "\n",
    "\n",
    "### Model Parameters\n",
    "Specify the number of topics to learn as well as the number of words to keep in the model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "\n",
    "# number of topics\n",
    "n_topics = 10\n",
    "\n",
    "# size of vocab\n",
    "n_words = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu,floatX=float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "%env THEANO_FLAGS=device=cpu,floatX=float64\n",
    "import theano\n",
    "\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from theano import shared\n",
    "import theano.tensor as tt\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "import pymc3 as pm\n",
    "from pymc3 import math as pmmath\n",
    "from pymc3 import Dirichlet\n",
    "from pymc3.distributions.transforms import t_stick_breaking\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.191s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.999s.\n"
     ]
    }
   ],
   "source": [
    "# The number of words in the vocabulary\n",
    "n_words = n_words\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_words,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs for training = 7920\n",
      "Number of docs for testing = 3394\n",
      "Number of tokens in training set = 384502\n",
      "Sparsity = 0.0255030303030303\n"
     ]
    }
   ],
   "source": [
    "n_samples_tr = round(tf.shape[0] * 0.7) # testing on 70%\n",
    "n_samples_te = tf.shape[0] - n_samples_tr\n",
    "docs_tr = tf[:n_samples_tr, :]\n",
    "docs_te = tf[n_samples_tr:, :]\n",
    "print('Number of docs for training = {}'.format(docs_tr.shape[0]))\n",
    "print('Number of docs for testing = {}'.format(docs_te.shape[0]))\n",
    "\n",
    "\n",
    "n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
    "print('Number of tokens in training set = {}'.format(n_tokens))\n",
    "print('Sparsity = {}'.format(\n",
    "    len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize Test-Train Split\n",
    "\n",
    "To increase random variability when repeatedly computing performance metrics, randomizing the train-test split can aid in assessing the spread (random variability) of performance metrics.\n",
    "\n",
    "As such I have randomized Train-Test-Split processes, to be called before each coherence calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Train_Test_Split(tf, p=0.7):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        Randomly train-test split scarce scipy matrix\n",
    "    \n",
    "    Parameters:\n",
    "        tf: clean dataset, type = scipy.sparse.csr.csr_matrix\n",
    "        p: percentage to assign to training data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shuffle\n",
    "    from sklearn.utils import shuffle\n",
    "    tf = shuffle(tf)\n",
    "    \n",
    "    n_samples_tr = round(tf.shape[0] * p) \n",
    "    n_samples_te = tf.shape[0] - n_samples_tr\n",
    "    \n",
    "    docs_tr = tf[:n_samples_tr, :]\n",
    "    docs_te = tf[n_samples_tr:, :]\n",
    "\n",
    "    n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
    "    Sparsity = len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1])\n",
    "\n",
    "    \n",
    "    results = {\n",
    "        'n_samples_tr': n_samples_tr,\n",
    "        'n_samples_te': n_samples_te,\n",
    "        'docs_tr': docs_tr,\n",
    "        'docs_te': docs_te,\n",
    "        'n_tokens': n_tokens,\n",
    "        'Sparsity': Sparsity\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate matrix\n",
    "test_tf = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Coherence Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/data.py:245: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Average Loss = 2.3883e+06: 100%|██████████| 10000/10000 [03:45<00:00, 44.67it/s]\n",
      "Finished [100%]: Average Loss = 2.3899e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.10896207824732\n"
     ]
    }
   ],
   "source": [
    "from LearnParameters import LearnParameters \n",
    "\n",
    "\n",
    "# results_pymc3 = LearnParameters.train_pymc3(docs_te, docs_tr, n_samples_te, n_samples_tr, n_words, n_topics, n_tokens)\n",
    "\n",
    "# results_Sklearn = LearnParameters.train_sklearn(docs_te, docs_tr, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence\n",
    "\n",
    "For each number of topics, run the coherence algorithm 10 times. To further increase the spread of plausible values, 'reshuffle' the training/testing documents.\n",
    "\n",
    "Thereafter a sampling distribution can be learnt about the characteristics of model coherence.\n",
    "\n",
    "\n",
    "The below algorthm:\n",
    "    - Randomly assigns a train-test-split\n",
    "    - Learns model parameters\n",
    "    - Computes topic model coherence\n",
    "    - Returns a list of coherence scores for both Pymc3 & Sklearn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from LearnParameters import LearnParameters \n",
    "from coherence_umass import coherence_umass\n",
    "# coherence_pymc3 = coherence_umass.coherence(data_samples, results_pymc3['beta'], feature_names)\n",
    "\n",
    "\n",
    "def coherence_list(tf, corpus, feature_names, n_topics, iters=10, n_words=1000, n_common_words=10, epsilon=1):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        A map of lists of coherence scores (for various train-test-splits & model parameter approximation)\n",
    "    \n",
    "    Parameters:\n",
    "        REQUIRED:\n",
    "            tf: unsplit, processed, term-freq matrix\n",
    "            corpus: unprocessed corpus of data\n",
    "            feature_names: list of most common n words (processed)\n",
    "            n_topics: number of topics to learn in the model\n",
    "        \n",
    "        OPTIONAL:\n",
    "            iters: number of times to iterate\n",
    "            n_words: no. of words to keep in the model vocabulary\n",
    "            n_common_words: number of 'most common words' to use to compute coherence,\n",
    "            epsilon: smoothing parameter for the conherence joint probability computation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_py = []\n",
    "    coherence_sk = []\n",
    "    t0 = time()\n",
    "        \n",
    "    for i in range(iters):\n",
    "        print('\\n Iteration: ', i)\n",
    "        \n",
    "        random_TTS = Random_Train_Test_Split(tf)\n",
    "        n_samples_tr = random_TTS['n_samples_tr']\n",
    "        n_samples_te = random_TTS['n_samples_te']\n",
    "        docs_te = random_TTS['docs_te']\n",
    "        docs_tr = random_TTS['docs_tr']\n",
    "        n_tokens = random_TTS['n_tokens']\n",
    "\n",
    "        # PyMC3\n",
    "        results_pymc3 = LearnParameters.train_pymc3(docs_te, docs_tr, n_samples_te, n_samples_tr, n_words, n_topics, n_tokens)\n",
    "        coherence_pymc3 = coherence_umass.coherence(data_samples, results_pymc3['beta'], feature_names, n_common_words=n_common_words, epsilon=epsilon)\n",
    "        coherence_py.append(coherence_pymc3)\n",
    "\n",
    "        # Sklearn\n",
    "        results_Sklearn = LearnParameters.train_sklearn(docs_te, docs_tr, n_topics)\n",
    "        coherence_Sklearn = coherence_umass.coherence(data_samples, results_Sklearn['beta'], feature_names, n_common_words=n_common_words, epsilon=epsilon)\n",
    "        coherence_sk.append(coherence_Sklearn)\n",
    "    \n",
    "    timer = time() - t0\n",
    "    \n",
    "    results = {\n",
    "        'pymc3 coherence': coherence_py,\n",
    "        'sklearn coherence': coherence_sk,\n",
    "        'runtime': timer\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Topics\n",
    "\n",
    "Perform the full calculation (from split to compute coherence) for a model that learns $10$ topics.\n",
    "\n",
    "Save the results as a _.pkl_ (pickle) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/data.py:245: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Average Loss = 2.4493e+06: 100%|██████████| 10000/10000 [03:55<00:00, 42.42it/s]\n",
      "Finished [100%]: Average Loss = 2.4521e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -5.966144554906394\n",
      "\n",
      " Iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/data.py:245: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Average Loss = 2.3275e+06: 100%|██████████| 10000/10000 [03:37<00:00, 46.06it/s]\n",
      "Finished [100%]: Average Loss = 2.3283e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.211724942216167\n",
      "\n",
      " Iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.2927e+06: 100%|██████████| 10000/10000 [03:32<00:00, 50.18it/s]\n",
      "Finished [100%]: Average Loss = 2.2916e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.1850092251448\n",
      "\n",
      " Iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4134e+06: 100%|██████████| 10000/10000 [03:48<00:00, 43.72it/s]\n",
      "Finished [100%]: Average Loss = 2.4146e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.295613775368426\n",
      "\n",
      " Iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.3276e+06: 100%|██████████| 10000/10000 [03:28<00:00, 47.93it/s]\n",
      "Finished [100%]: Average Loss = 2.3284e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -5.9001408152260035\n",
      "\n",
      " Iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.2671e+06: 100%|██████████| 10000/10000 [03:29<00:00, 47.64it/s]\n",
      "Finished [100%]: Average Loss = 2.2671e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.23887260630289\n",
      "\n",
      " Iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.445e+06: 100%|██████████| 10000/10000 [03:54<00:00, 42.60it/s]\n",
      "Finished [100%]: Average Loss = 2.4456e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.293714509667282\n",
      "\n",
      " Iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4209e+06: 100%|██████████| 10000/10000 [03:27<00:00, 47.97it/s]\n",
      "Finished [100%]: Average Loss = 2.4211e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.191410225644912\n",
      "\n",
      " Iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4431e+06: 100%|██████████| 10000/10000 [03:38<00:00, 47.39it/s]\n",
      "Finished [100%]: Average Loss = 2.4402e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.127480592896584\n",
      "\n",
      " Iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4462e+06: 100%|██████████| 10000/10000 [03:38<00:00, 45.76it/s]\n",
      "Finished [100%]: Average Loss = 2.4487e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.372324465737378\n"
     ]
    }
   ],
   "source": [
    "coherence_10Topics = coherence_list(tf, data_samples, feature_names, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save times dict\n",
    "import pickle\n",
    "pickle_out = open('coherence_10Topics.pkl', 'wb')\n",
    "pickle.dump(coherence_10Topics, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pymc3 coherence', 'sklearn coherence', 'runtime'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_10Topics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25862.239357948303"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_10Topics['pymc3 coherence']\n",
    "coherence_10Topics['runtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.183955377207862"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_10Topics['runtime'] / 60 / 60 # hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Topics\n",
    "\n",
    "Perform the full calculation (from split to compute coherence) for a model that learns $20$ topics.\n",
    "\n",
    "Save the results as a _.pkl_ (pickle) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/data.py:245: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Average Loss = 2.4369e+06: 100%|██████████| 10000/10000 [05:31<00:00, 34.56it/s]\n",
      "Finished [100%]: Average Loss = 2.4352e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.269980636601577\n",
      "\n",
      " Iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/data.py:245: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Average Loss = 2.496e+06: 100%|██████████| 10000/10000 [05:34<00:00, 31.22it/s]\n",
      "Finished [100%]: Average Loss = 2.4968e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.293237777205665\n",
      "\n",
      " Iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4688e+06: 100%|██████████| 10000/10000 [05:14<00:00, 31.79it/s]\n",
      "Finished [100%]: Average Loss = 2.4686e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.046163368039833\n",
      "\n",
      " Iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4799e+06: 100%|██████████| 10000/10000 [06:14<00:00, 31.73it/s]\n",
      "Finished [100%]: Average Loss = 2.4792e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.0391100107719335\n",
      "\n",
      " Iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.413e+06: 100%|██████████| 10000/10000 [05:44<00:00, 29.02it/s]\n",
      "Finished [100%]: Average Loss = 2.4129e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.0499330885094\n",
      "\n",
      " Iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4696e+06: 100%|██████████| 10000/10000 [05:44<00:00, 29.06it/s]\n",
      "Finished [100%]: Average Loss = 2.4715e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.015767036916389\n",
      "\n",
      " Iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4727e+06: 100%|██████████| 10000/10000 [05:20<00:00, 31.20it/s]\n",
      "Finished [100%]: Average Loss = 2.472e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.076921527381877\n",
      "\n",
      " Iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4952e+06: 100%|██████████| 10000/10000 [05:50<00:00, 28.50it/s]\n",
      "Finished [100%]: Average Loss = 2.4968e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.524978126225958\n",
      "\n",
      " Iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4118e+06: 100%|██████████| 10000/10000 [05:26<00:00, 30.62it/s]\n",
      "Finished [100%]: Average Loss = 2.4119e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -5.904192533848899\n",
      "\n",
      " Iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.5094e+06: 100%|██████████| 10000/10000 [05:28<00:00, 30.47it/s]\n",
      "Finished [100%]: Average Loss = 2.5093e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.069923641639302\n"
     ]
    }
   ],
   "source": [
    "coherence_20Topics = coherence_list(tf, data_samples, feature_names, n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save times dict\n",
    "import pickle\n",
    "pickle_out = open('coherence_20Topics.pkl', 'wb')\n",
    "pickle.dump(coherence_20Topics, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Topics\n",
    "\n",
    "Perform the full calculation (from split to compute coherence) for a model that learns $15$ topics.\n",
    "\n",
    "Save the results as a _.pkl_ (pickle) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.423e+06: 100%|██████████| 10000/10000 [9:48:58<00:00,  3.53s/it]       \n",
      "Finished [100%]: Average Loss = 2.4233e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.218041493394998\n",
      "\n",
      " Iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.328e+06: 100%|██████████| 10000/10000 [04:25<00:00, 37.72it/s]\n",
      "Finished [100%]: Average Loss = 2.3265e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.070801035059394\n",
      "\n",
      " Iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.3713e+06: 100%|██████████| 10000/10000 [04:37<00:00, 36.00it/s]\n",
      "Finished [100%]: Average Loss = 2.3719e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.530034771986264\n",
      "\n",
      " Iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4807e+06: 100%|██████████| 10000/10000 [04:28<00:00, 37.22it/s]\n",
      "Finished [100%]: Average Loss = 2.4772e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -5.990515072589926\n",
      "\n",
      " Iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4457e+06: 100%|██████████| 10000/10000 [04:38<00:00, 35.86it/s]\n",
      "Finished [100%]: Average Loss = 2.4433e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.1529731952533995\n",
      "\n",
      " Iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.3339e+06: 100%|██████████| 10000/10000 [04:53<00:00, 34.11it/s]\n",
      "Finished [100%]: Average Loss = 2.3337e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.172745250049275\n",
      "\n",
      " Iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.5308e+06: 100%|██████████| 10000/10000 [15:57<00:00, 10.44it/s]\n",
      "Finished [100%]: Average Loss = 2.5307e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.100908215720134\n",
      "\n",
      " Iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4651e+06: 100%|██████████| 10000/10000 [05:16<00:00, 31.58it/s]\n",
      "Finished [100%]: Average Loss = 2.4634e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.176964461835821\n",
      "\n",
      " Iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4209e+06: 100%|██████████| 10000/10000 [04:27<00:00, 37.41it/s]\n",
      "Finished [100%]: Average Loss = 2.4211e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.176914747822377\n",
      "\n",
      " Iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 2.4454e+06: 100%|██████████| 10000/10000 [04:26<00:00, 37.47it/s]\n",
      "Finished [100%]: Average Loss = 2.448e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive log prob (pm3) = -6.110171294190219\n"
     ]
    }
   ],
   "source": [
    "coherence_15Topics = coherence_list(tf, data_samples, feature_names, n_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save times dict\n",
    "import pickle\n",
    "pickle_out = open('coherence_15Topics.pkl', 'wb')\n",
    "pickle.dump(coherence_15Topics, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
