{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.B: LDA Autoencoding Variational Bayes\n",
    "\n",
    "```\n",
    "Author:\n",
    "Zach Wolpe\n",
    "zachcolinwolpe@gmail.com\n",
    "www.zachwolpe.com\n",
    "```\n",
    "\n",
    "Identical to Experiment 1.A - however utilizing Autoencoding for the LDA learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu,floatX=float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "%env THEANO_FLAGS=device=cpu,floatX=float64\n",
    "import theano\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from theano import shared\n",
    "import theano.tensor as tt\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "import pymc3 as pm\n",
    "from pymc3 import math as pmmath\n",
    "from pymc3 import Dirichlet\n",
    "from pymc3.distributions.transforms import t_stick_breaking\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "To eliminate the possibility of decrepencies in model performance arising from unrelated differences, the same dataset & processing techniques are employed as _'Experiment 1.a'_.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "I've used a sub-section of the _20 News Groups_ dataset (a dataset containing news headlines). This subset contains 11'000 headlines. This version of the dataset contains about 11k newsgroups posts from 20 different topics.\n",
    "\n",
    "The topics are labeled and as such we can compare topics with forecasted results.\n",
    "\n",
    "\n",
    "## Extensive Text Processing - Gensim\n",
    "\n",
    "### Download dataset and stopwords\n",
    "\n",
    "##### Example dataset:\n",
    "I've used a sub-section of the _20 News Groups_ dataset (a dataset containing news headlines). This subset contains 11'000 headlines. This version of the dataset contains about 11k newsgroups posts from 20 different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "import theano\n",
    "from time import time\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachcolinwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  target  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "\n",
       "                 target_names  \n",
       "0                   rec.autos  \n",
       "1       comp.sys.mac.hardware  \n",
       "10            rec.motorcycles  \n",
       "100              misc.forsale  \n",
       "1000  comp.os.ms-windows.misc  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# NLTK Stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "\n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bigram and Trigram Models\n",
    "\n",
    "Two words that frequently occur together form a bigram, three a trigram.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are _min_count_ and _threshold_. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 's', 'thing', 'car', 'nntp_poste', 'host', 'umd', 'organization', 'university', 'maryland_college', 'park', 'line', 'wonder', 'anyone', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# call the above functions\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them.\n",
    "\n",
    "Now the dictionary and term freq matrix are ready to feed into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 5), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Comparison\n",
    "\n",
    "In conducting a variation bayes experimentation, utilising the small - approx 11'000 doc - corpus (of 20 newsgroups), I will compare Gensim LDA vs AEVB based on the following metrix:\n",
    "    - coherence score\n",
    "    - computational speed\n",
    "    - topic inspection (top 10 elements)\n",
    "    - Hungarian method\n",
    "    \n",
    "# AutoEncoding LDA implementations\n",
    "\n",
    "Fitting the LDA using an Autoencoder.\n",
    "\n",
    "## Term frequency document\n",
    "The original Auto-Encoder tutorial utilizes a term frequence document for training however a slightly different document structure is used here. \n",
    "\n",
    "The processed text documennts are now converted back to text corpus, however keeping the processed tokens as apposed to the original unprocessed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    where s thing car nntp_poste host umd organiza...\n",
       "1    clock poll final call summary final call si cl...\n",
       "2    irwin arnstein recommendation duc summary s wo...\n",
       "3    tsung kun chen software forsale lot nntp_poste...\n",
       "4    lindbergh diamond_ss win mouse_cursor organiza...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term freq doc\n",
    "data = pd.Series(data_lemmatized)\n",
    "\n",
    "data = pd.Series([\" \".join(i) for i in data])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Dataset\n",
    "\n",
    "Consider another dataset:\n",
    "\n",
    "Employee Reviews from FANG companies. 120 datapoints contain no 'summary' - and are disgarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67409, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>dates</th>\n",
       "      <th>job-title</th>\n",
       "      <th>summary</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "      <th>advice-to-mgmt</th>\n",
       "      <th>overall-ratings</th>\n",
       "      <th>work-balance-stars</th>\n",
       "      <th>culture-values-stars</th>\n",
       "      <th>carrer-opportunities-stars</th>\n",
       "      <th>comp-benefit-stars</th>\n",
       "      <th>senior-mangemnet-stars</th>\n",
       "      <th>helpful-count</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google</td>\n",
       "      <td>none</td>\n",
       "      <td>Dec 11, 2018</td>\n",
       "      <td>Current Employee - Anonymous Employee</td>\n",
       "      <td>Best Company to work for</td>\n",
       "      <td>People are smart and friendly</td>\n",
       "      <td>Bureaucracy is slowing things down</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/Google-Revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>google</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Jun 21, 2013</td>\n",
       "      <td>Former Employee - Program Manager</td>\n",
       "      <td>Moving at the speed of light, burn out is inev...</td>\n",
       "      <td>1) Food, food, food. 15+ cafes on main campus ...</td>\n",
       "      <td>1) Work/life balance. What balance? All those ...</td>\n",
       "      <td>1) Don't dismiss emotional intelligence and ad...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2094</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/Google-Revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>google</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>May 10, 2014</td>\n",
       "      <td>Current Employee - Software Engineer III</td>\n",
       "      <td>Great balance between big-company security and...</td>\n",
       "      <td>* If you're a software engineer, you're among ...</td>\n",
       "      <td>* It *is* becoming larger, and with it comes g...</td>\n",
       "      <td>Keep the focus on the user. Everything else wi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>949</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/Google-Revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>google</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Feb 8, 2015</td>\n",
       "      <td>Current Employee - Anonymous Employee</td>\n",
       "      <td>The best place I've worked and also the most d...</td>\n",
       "      <td>You can't find a more well-regarded company th...</td>\n",
       "      <td>I live in SF so the commute can take between 1...</td>\n",
       "      <td>Keep on NOT micromanaging - that is a huge ben...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>498</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/Google-Revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>google</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Jul 19, 2018</td>\n",
       "      <td>Former Employee - Software Engineer</td>\n",
       "      <td>Unique, one of a kind dream job</td>\n",
       "      <td>Google is a world of its own. At every other c...</td>\n",
       "      <td>If you don't work in MTV (HQ), you will be giv...</td>\n",
       "      <td>Promote managers into management for their man...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/Google-Revie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 company           location          dates  \\\n",
       "0           1  google               none   Dec 11, 2018   \n",
       "1           2  google  Mountain View, CA   Jun 21, 2013   \n",
       "2           3  google       New York, NY   May 10, 2014   \n",
       "3           4  google  Mountain View, CA    Feb 8, 2015   \n",
       "4           5  google    Los Angeles, CA   Jul 19, 2018   \n",
       "\n",
       "                                  job-title  \\\n",
       "0     Current Employee - Anonymous Employee   \n",
       "1         Former Employee - Program Manager   \n",
       "2  Current Employee - Software Engineer III   \n",
       "3     Current Employee - Anonymous Employee   \n",
       "4       Former Employee - Software Engineer   \n",
       "\n",
       "                                             summary  \\\n",
       "0                           Best Company to work for   \n",
       "1  Moving at the speed of light, burn out is inev...   \n",
       "2  Great balance between big-company security and...   \n",
       "3  The best place I've worked and also the most d...   \n",
       "4                    Unique, one of a kind dream job   \n",
       "\n",
       "                                                pros  \\\n",
       "0                      People are smart and friendly   \n",
       "1  1) Food, food, food. 15+ cafes on main campus ...   \n",
       "2  * If you're a software engineer, you're among ...   \n",
       "3  You can't find a more well-regarded company th...   \n",
       "4  Google is a world of its own. At every other c...   \n",
       "\n",
       "                                                cons  \\\n",
       "0                 Bureaucracy is slowing things down   \n",
       "1  1) Work/life balance. What balance? All those ...   \n",
       "2  * It *is* becoming larger, and with it comes g...   \n",
       "3  I live in SF so the commute can take between 1...   \n",
       "4  If you don't work in MTV (HQ), you will be giv...   \n",
       "\n",
       "                                      advice-to-mgmt  overall-ratings  \\\n",
       "0                                               none              5.0   \n",
       "1  1) Don't dismiss emotional intelligence and ad...              4.0   \n",
       "2  Keep the focus on the user. Everything else wi...              5.0   \n",
       "3  Keep on NOT micromanaging - that is a huge ben...              5.0   \n",
       "4  Promote managers into management for their man...              5.0   \n",
       "\n",
       "  work-balance-stars culture-values-stars carrer-opportunities-stars  \\\n",
       "0                4.0                  5.0                        5.0   \n",
       "1                2.0                  3.0                        3.0   \n",
       "2                5.0                  4.0                        5.0   \n",
       "3                2.0                  5.0                        5.0   \n",
       "4                5.0                  5.0                        5.0   \n",
       "\n",
       "  comp-benefit-stars senior-mangemnet-stars  helpful-count  \\\n",
       "0                4.0                    5.0              0   \n",
       "1                5.0                    3.0           2094   \n",
       "2                5.0                    4.0            949   \n",
       "3                4.0                    5.0            498   \n",
       "4                5.0                    5.0             49   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.glassdoor.com/Reviews/Google-Revie...  \n",
       "1  https://www.glassdoor.com/Reviews/Google-Revie...  \n",
       "2  https://www.glassdoor.com/Reviews/Google-Revie...  \n",
       "3  https://www.glassdoor.com/Reviews/Google-Revie...  \n",
       "4  https://www.glassdoor.com/Reviews/Google-Revie...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('../datasets/employee_reviews.csv')\n",
    "reviews = reviews[reviews.summary.notnull()]\n",
    "\n",
    "summary = reviews.summary\n",
    "\n",
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create term frequency document \n",
    "\n",
    "The redesigned corpus is now vectorized to produce a sparse term-frequency matrix.\n",
    "\n",
    "We are able to limit the number of words here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# processed data\n",
    "# data\n",
    "\n",
    "# try with raw data\n",
    "# df\n",
    "\n",
    "n_words = round(corpus[-1][-1][0], -4) # Na      # total number of words in the gensim dictionary / corpus\n",
    "n_words = 37724 #NA \n",
    "# n_words = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.738s.\n",
      "Extracting tf features for LDA...\n",
      "done in 3.509s.\n"
     ]
    }
   ],
   "source": [
    "# The number of words in the vocabulary\n",
    "n_words = 1000\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_words,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "Split the data to improve reliability of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs for training = 7920\n",
      "Number of docs for testing = 3394\n",
      "Number of tokens in training set = 384502\n",
      "Sparsity = 0.0255030303030303\n"
     ]
    }
   ],
   "source": [
    "# We split the whole documents into training and test sets. \n",
    "# The number of tokens in the training set is 480K. \n",
    "# Sparsity of the term-frequency document matrix is 0.025%, which implies almost all components in the term-frequency matrix is zero\n",
    "\n",
    "\n",
    "n_samples_tr = round(tf.shape[0] * 0.7) # testing on 70%\n",
    "n_samples_te = tf.shape[0] - n_samples_tr\n",
    "docs_tr = tf[:n_samples_tr, :]\n",
    "docs_te = tf[n_samples_tr:, :]\n",
    "print('Number of docs for training = {}'.format(docs_tr.shape[0]))\n",
    "print('Number of docs for testing = {}'.format(docs_te.shape[0]))\n",
    "\n",
    "\n",
    "n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
    "print('Number of tokens in training set = {}'.format(n_tokens))\n",
    "print('Sparsity = {}'.format(\n",
    "    len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-likelihood of documents for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_lda_doc(beta, theta):\n",
    "  \n",
    "  \"\"\"Returns the log-likelihood function for given documents.\n",
    "  \n",
    "  K : number of topics in the model\n",
    "  V : number of words (size of vocabulary)\n",
    "  D : number of documents (in a mini-batch)\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  beta : tensor (K x V)\n",
    "      Word distribution.\n",
    "  theta : tensor (D x K)\n",
    "      Topic distributions for the documents.\n",
    "  \"\"\"\n",
    "  \n",
    "  def ll_docs_f(docs):\n",
    "    \n",
    "    dixs, vixs = docs.nonzero()\n",
    "    vfreqs = docs[dixs, vixs]\n",
    "    ll_docs = vfreqs * pmmath.logsumexp(\n",
    "          tt.log(theta[dixs]) + tt.log(beta.T[vixs]), axis=1).ravel()\n",
    "      \n",
    "    # Per-word log-likelihood times no. of tokens in the whole dataset\n",
    "    return tt.sum(ll_docs) / (tt.sum(vfreqs)+1e-9) * n_tokens\n",
    "\n",
    "  return ll_docs_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "# we have sparse dataset. It's better to have dence batch so that all words accure there\n",
    "minibatch_size = 128\n",
    "\n",
    "# defining minibatch\n",
    "doc_t_minibatch = pm.Minibatch(docs_tr.toarray(), minibatch_size)\n",
    "doc_t = shared(docs_tr.toarray()[:minibatch_size])\n",
    "\n",
    "with pm.Model() as model:\n",
    "  theta = Dirichlet('theta', a=pm.floatX((1.0 / n_topics) * np.ones((minibatch_size, n_topics))),\n",
    "                   shape=(minibatch_size, n_topics), transform=t_stick_breaking(1e-9),\n",
    "                   # do not forget scaling\n",
    "                   total_size = n_samples_tr)\n",
    "  beta = Dirichlet('beta', a=pm.floatX((1.0 / n_topics) * np.ones((n_topics, n_words))),\n",
    "                 shape=(n_topics, n_words), transform=t_stick_breaking(1e-9))\n",
    "  # Note, that we defined likelihood with scaling, so here we need no additional `total_size` kwarg\n",
    "  doc = pm.DensityDist('doc', logp_lda_doc(beta, theta), observed=doc_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LDAEncoder:\n",
    "  \"\"\"Encode (term-frequency) document vectors to variational means and (log-transformed) stds.\n",
    "  \"\"\"\n",
    "  def __init__(self, n_words, n_hidden, n_topics, p_corruption=0, random_seed=1):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    self.n_words = n_words\n",
    "    self.n_hidden = n_hidden\n",
    "    self.n_topics = n_topics\n",
    "    self.w0 = shared(0.01 * rng.randn(n_words, n_hidden).ravel(), name='w0')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class LDAEncoder:\n",
    "    \"\"\"Encode (term-frequency) document vectors to variational means and (log-transformed) stds.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_words, n_hidden, n_topics, p_corruption=0, random_seed=1):\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.n_words = n_words\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_topics = n_topics\n",
    "        self.w0 = shared(0.01 * rng.randn(n_words, n_hidden).ravel(), name='w0')\n",
    "        self.b0 = shared(0.01 * rng.randn(n_hidden), name='b0')\n",
    "        self.w1 = shared(0.01 * rng.randn(n_hidden, 2 * (n_topics - 1)).ravel(), name='w1')\n",
    "        self.b1 = shared(0.01 * rng.randn(2 * (n_topics - 1)), name='b1')\n",
    "        self.rng = MRG_RandomStreams(seed=random_seed)\n",
    "        self.p_corruption = p_corruption\n",
    "\n",
    "    def encode(self, xs):\n",
    "        if 0 < self.p_corruption:\n",
    "            dixs, vixs = xs.nonzero()\n",
    "            mask = tt.set_subtensor(\n",
    "                tt.zeros_like(xs)[dixs, vixs],\n",
    "                self.rng.binomial(size=dixs.shape, n=1, p=1-self.p_corruption)\n",
    "            )\n",
    "            xs_ = xs * mask\n",
    "        else:\n",
    "            xs_ = xs\n",
    "\n",
    "        w0 = self.w0.reshape((self.n_words, self.n_hidden))\n",
    "        w1 = self.w1.reshape((self.n_hidden, 2 * (self.n_topics - 1)))\n",
    "        hs = tt.tanh(xs_.dot(w0) + self.b0)\n",
    "        zs = hs.dot(w1) + self.b1\n",
    "        zs_mean = zs[:, :(self.n_topics - 1)]\n",
    "        zs_rho = zs[:, (self.n_topics - 1):]\n",
    "        return {'mu': zs_mean, 'rho':zs_rho}\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.w0, self.b0, self.w1, self.b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.641329326498145"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape\n",
    "np.mean(np.sum(tf, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(theta,\n",
       "              {'mu': Subtensor{::, :int64:}.0,\n",
       "               'rho': Subtensor{::, int64::}.0})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LDAEncoder(n_words=n_words, n_hidden=100, n_topics=n_topics, p_corruption=0.0)\n",
    "local_RVs = OrderedDict([(theta, encoder.encode(doc_t))])\n",
    "local_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[w0, b0, w1, b1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_params = encoder.get_params()\n",
    "encoder_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEVB with ADVI\n",
    "\n",
    "# Additional Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacker_thetas = []\n",
    "tacker_betas = []\n",
    "tacker_thetas_1 = []\n",
    "\n",
    "def my_callback(a,h,i):\n",
    "    tacker_thetas.append(pm.sample_approx(approx, draws=20000)['theta'].mean(axis=0))\n",
    "    tacker_thetas_1.append(pm.sample_approx(approx, draws=1)['theta'].mean(axis=0))\n",
    "    a = a\n",
    "    h = h\n",
    "    i = i \n",
    "    \n",
    "  #   tacker_betas.append(approx.model.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 3.3176e+06: 100%|██████████| 5/5 [23:32<00:00, 295.70s/it]\n",
      "Finished [100%]: Loss = 3.1216e+06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymc3.variational.approximations.MeanField at 0x1c302a2668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "η = .1\n",
    "s = shared(η)\n",
    "def reduce_rate(a, h, i):\n",
    "    s.set_value(η/((i/minibatch_size)+1)**.7)\n",
    "\n",
    "with model:\n",
    "    approx = pm.MeanField(local_rv=local_RVs)\n",
    "    approx.scale_cost_to_minibatch = False\n",
    "    inference = pm.KLqp(approx)\n",
    "inference.fit(5, callbacks=[reduce_rate, my_callback], obj_optimizer=pm.sgd(learning_rate=s),\n",
    "              more_obj_params=encoder_params, total_grad_norm_constraint=200,\n",
    "              more_replacements={doc_t:doc_t_minibatch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.23173479e-09, 1.89343867e-09, 9.99999994e-01, 3.85012306e-15,\n",
       "       1.00024939e-18, 1.99809576e-14, 3.57581826e-11, 3.86070885e-11,\n",
       "       9.25610974e-10, 9.25611051e-19])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tacker_thetas_1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__break___\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.52521 0.47479 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.     ]\n",
      "[0.13497 0.      0.86503 0.      0.      0.      0.      0.      0.\n",
      " 0.     ]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "__break___\n",
      "[1.0000e-05 4.0000e-05 1.9905e-01 0.0000e+00 8.0091e-01 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "[2.0000e-05 0.0000e+00 5.6290e-02 9.4368e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "[2.0000e-05 1.0000e-05 4.4000e-04 0.0000e+00 9.9008e-01 9.4400e-03\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "[2.0000e-05 0.0000e+00 6.0360e-01 3.9638e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "[1.0000e-05 0.0000e+00 4.1910e-02 1.0000e-04 9.5797e-01 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "__break___\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.01976 0.98024 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.     ]\n",
      "[0.34569 0.00747 0.64684 0.      0.      0.      0.      0.      0.\n",
      " 0.     ]\n",
      "__break___\n",
      "[4.0000e-05 3.7700e-03 1.7100e-03 9.9449e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "[6.0000e-05 1.0000e-05 1.1000e-04 3.1280e-02 0.0000e+00 0.0000e+00\n",
      " 5.5415e-01 1.7190e-02 3.9590e-01 1.3000e-03]\n",
      "[6.1000e-04 5.0000e-04 2.2210e-02 0.0000e+00 3.1889e-01 5.0000e-05\n",
      " 3.7086e-01 5.7260e-02 2.2787e-01 1.7400e-03]\n",
      "[9.0000e-05 2.1000e-04 8.1027e-01 3.0000e-05 1.5260e-02 0.0000e+00\n",
      " 1.1318e-01 3.1600e-03 5.7730e-02 7.0000e-05]\n",
      "[1.0000e-04 1.0000e-04 9.8297e-01 0.0000e+00 3.0000e-05 1.5630e-02\n",
      " 6.6000e-04 3.0000e-05 4.7000e-04 0.0000e+00]\n",
      "__break___\n",
      "[0.0000e+00 9.9998e-01 2.0000e-05 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "np.round(tacker_thetas_1[0][0], 5)\n",
    "\n",
    "for j in range(len(tacker_thetas_1)):\n",
    "    \n",
    "    print('__break___')\n",
    "    \n",
    "    for i in range(len(tacker_thetas_1)):\n",
    "        print(np.round(tacker_thetas_1[j][i], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation{MeanFieldGroup[None, 9] & MeanFieldGroup[9990]}\n"
     ]
    }
   ],
   "source": [
    "print(approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(approx.hist[10:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of characteristic words of topics based on posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: use file windows key program drive does bit using like\n",
      "Topic #1: people god think don just know like say time said\n",
      "Topic #2: 00 10 11 15 20 db 12 55 16 17\n",
      "Topic #3: ax max g9v b8f 75u a86 1t bhj 145 pl\n",
      "Topic #4: just like good don car time year think game team\n",
      "Topic #5: edu space com information nasa mail anonymous new university data\n",
      "Topic #6: know don just like think does people use good thanks\n",
      "Topic #7: know just like don does thanks good mail ve think\n",
      "Topic #8: know don like just does thanks think good new edu\n",
      "Topic #9: like know just don does thanks good think new edu\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    for i in range(len(beta)):\n",
    "        print((\"Topic #%d: \" % i) + \" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "doc_t.set_value(docs_tr.toarray())\n",
    "samples = pm.sample_approx(approx, draws=100)\n",
    "beta_pymc3 = samples['beta'].mean(axis=0)\n",
    "\n",
    "print_top_words(beta_pymc3, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 1.45 s, total: 1min 4s\n",
      "Wall time: 46.4 s\n",
      "Topic #0: gun people armenian armenians turkish state states said 000 war\n",
      "Topic #1: mr law people government use don president stephanopoulos right know\n",
      "Topic #2: space nasa data program output launch science research entry earth\n",
      "Topic #3: key car chip keys used clipper bit bike use wire\n",
      "Topic #4: edu file com mail available files information ftp image list\n",
      "Topic #5: god people does jesus believe think say don know just\n",
      "Topic #6: windows db drive use thanks does problem know card like\n",
      "Topic #7: ax max g9v pl b8f 75u a86 bhj 1t 34u\n",
      "Topic #8: just don like think know time good people ve didn\n",
      "Topic #9: 00 10 15 20 12 11 50 16 25 new\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "%time lda.fit(docs_tr)\n",
    "beta_sklearn = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print_top_words(beta_sklearn, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn_20NewsGroups.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save sklearn model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lda, 'sklearn_20NewsGroups.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pp(ws, thetas, beta, wix):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws: ndarray (N,)\n",
    "        Number of times the held-out word appeared in N documents.\n",
    "    thetas: ndarray, shape=(N, K)\n",
    "        Topic distributions for N documents.\n",
    "    beta: ndarray, shape=(K, V)\n",
    "        Word distributions for K topics.\n",
    "    wix: int\n",
    "        Index of the held-out word\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Log probability of held-out words.\n",
    "    \"\"\"\n",
    "    return ws * np.log(thetas.dot(beta[:, wix]))\n",
    "\n",
    "def eval_lda(transform, beta, docs_te, wixs):\n",
    "    \"\"\"Evaluate LDA model by log predictive probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transform: Python function\n",
    "        Transform document vectors to posterior mean of topic proportions.\n",
    "    wixs: iterable of int\n",
    "        Word indices to be held-out.\n",
    "    \"\"\"\n",
    "    lpss = []\n",
    "    docs_ = deepcopy(docs_te)\n",
    "    thetass = []\n",
    "    wss = []\n",
    "    total_words = 0\n",
    "    for wix in wixs:\n",
    "        ws = docs_te[:, wix].ravel()\n",
    "        if 0 < ws.sum():\n",
    "            # Hold-out\n",
    "            docs_[:, wix] = 0\n",
    "\n",
    "            # Topic distributions\n",
    "            thetas = transform(docs_)\n",
    "\n",
    "            # Predictive log probability\n",
    "            lpss.append(calc_pp(ws, thetas, beta, wix))\n",
    "\n",
    "            docs_[:, wix] = ws\n",
    "            thetass.append(thetas)\n",
    "            wss.append(ws)\n",
    "            total_words += ws.sum()\n",
    "        else:\n",
    "            thetass.append(None)\n",
    "            wss.append(None)\n",
    "\n",
    "    # Log-probability\n",
    "    lp = np.sum(np.hstack(lpss)) / total_words\n",
    "\n",
    "    return {\n",
    "        'lp': lp,\n",
    "        'thetass': thetass,\n",
    "        'beta': beta,\n",
    "        'wss': wss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tt.matrix(dtype='int64')\n",
    "sample_vi_theta = theano.function(\n",
    "    [inp],\n",
    "    approx.sample_node(approx.model.theta, 100,  more_replacements={doc_t: inp}).mean(0)\n",
    ")\n",
    "def transform_pymc3(docs):\n",
    "    return sample_vi_theta(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 3.01 s, total: 1min 27s\n",
      "Wall time: 56.1 s\n",
      "Predictive log prob (pm3) = -6.10896207824732\n"
     ]
    }
   ],
   "source": [
    "%time result_pymc3 = eval_lda(transform_pymc3, beta_pymc3, docs_te.toarray(), np.arange(100))\n",
    "print('Predictive log prob (pm3) = {}'.format(result_pymc3['lp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 57s, sys: 3.5 s, total: 6min\n",
      "Wall time: 3min 37s\n",
      "Predictive log prob (sklearn) = -6.007546255948269\n"
     ]
    }
   ],
   "source": [
    "def transform_sklearn(docs):\n",
    "    thetas = lda.transform(docs)\n",
    "    return thetas / thetas.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "%time result_sklearn = eval_lda(transform_sklearn, beta_sklearn, docs_te.toarray(), np.arange(100))\n",
    "print('Predictive log prob (sklearn) = {}'.format(result_sklearn['lp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the pymc3 results\n",
    "\n",
    "Thetas, Betas, lp's & wss (weighted sum of squares) is given as 'result_pymc3'. Below I save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['result_pymc3.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pymc3.keys()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(result_pymc3, 'result_pymc3.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
